# üîç Guia Pr√°tico de Observabilidade - ToolsChallenge

> **Guia hands-on** para usar Grafana, Prometheus, Jaeger e Actuator no dia a dia do desenvolvimento

---

## üìã √çndice R√°pido

1. [Stack de Observabilidade](#-stack-de-observabilidade)
2. [Grafana - Dashboards Visuais](#-grafana---dashboards-visuais)
3. [Prometheus - Queries e M√©tricas](#-prometheus---queries-e-m√©tricas)
4. [Jaeger - Distributed Tracing](#-jaeger---distributed-tracing)
5. [Actuator - Health Checks](#-actuator---health-checks)
6. [Casos de Uso Pr√°ticos](#-casos-de-uso-pr√°ticos)
7. [Troubleshooting com Observabilidade](#-troubleshooting-com-observabilidade)

---

## üéØ Stack de Observabilidade

### **URLs de Acesso R√°pido**

| Ferramenta | URL | Credenciais | Prop√≥sito |
|------------|-----|-------------|-----------|
| **Grafana** | http://localhost:3000 | `admin` / `admin123` | Dashboards visuais, alertas |
| **Prometheus** | http://localhost:9090 | - | Queries PromQL, m√©tricas raw |
| **Jaeger** | http://localhost:16686 | - | Traces distribu√≠dos, debugging |
| **Actuator** | http://localhost:8080/atuador | - | Health checks, m√©tricas API |
| **Swagger** | http://localhost:8080/swagger-ui.html | - | Documenta√ß√£o interativa API |
| **Kafka UI** | http://localhost:8081 | - | Monitorar t√≥picos Kafka |

### **Quando usar cada ferramenta?**

| Situa√ß√£o | Ferramenta | Por qu√™? |
|----------|-----------|----------|
| "A API est√° lenta" | **Grafana** ‚Üí HTTP Metrics | Lat√™ncia p95/p99 por endpoint |
| "Quantos pagamentos foram criados hoje?" | **Grafana** ‚Üí Business Metrics | Counters de neg√≥cio |
| "Circuit Breaker est√° abrindo muito" | **Grafana** ‚Üí Resilience4j Dashboard | Estados, taxas de falha |
| "Preciso de uma query customizada" | **Prometheus** | PromQL queries ad-hoc |
| "Onde est√° travando a requisi√ß√£o?" | **Jaeger** | Traces com timings por span |
| "A aplica√ß√£o est√° saud√°vel?" | **Actuator** `/health` | Status de DB, Redis, Kafka |
| "Quanto de mem√≥ria JVM est√° usando?" | **Grafana** ‚Üí JVM Micrometer | Heap, GC, threads |

---

## üìä Grafana - Dashboards Visuais

### **1. Acessar Grafana**

```bash
# Abrir no navegador
http://localhost:3000

# Login
Usu√°rio: admin
Senha: admin123
```

**Primeira vez**: Grafana pedir√° para trocar a senha (pode pular clicando "Skip").

---

### **2. Navegar pelos Dashboards**

**Menu lateral esquerdo** ‚Üí **Dashboards** (√≠cone de 4 quadrados)

Voc√™ ver√° **5 dashboards provisionados**:

#### **üìà Dashboards Community (3)**

**1. JVM Micrometer** (`jvm_micrometer_dashboard`)

**Para que serve**: Monitorar sa√∫de da JVM (mem√≥ria, GC, threads)

**Principais pain√©is**:
- **JVM Memory Pools**: Heap vs Non-Heap, Eden, Survivor, Old Gen
- **Garbage Collection**: Contagem de GC, tempo de pausa
- **Threads**: Threads ativas, daemon, pico
- **CPU Usage**: Uso de CPU do processo

**Quando usar**:
- ‚úÖ Investigar OutOfMemoryError
- ‚úÖ Analisar performance de GC
- ‚úÖ Detectar memory leaks (heap crescendo sem parar)
- ‚úÖ Verificar se threads est√£o aumentando (pode indicar leak)

**Como interpretar**:
```
üü¢ Heap usado < 70% do m√°ximo ‚Üí OK
üü° Heap usado 70-85% ‚Üí Aten√ß√£o (pode precisar mais mem√≥ria)
üî¥ Heap usado > 90% ‚Üí Cr√≠tico (risco de OOM)

üü¢ GC pause < 100ms ‚Üí OK
üü° GC pause 100-500ms ‚Üí Aten√ß√£o (pode afetar lat√™ncia)
üî¥ GC pause > 500ms ‚Üí Cr√≠tico (usu√°rio vai perceber)
```

---

**2. Spring Boot Statistics** (`spring_boot_21`)

**Para que serve**: Vis√£o geral da aplica√ß√£o Spring Boot

**Principais pain√©is**:
- **HTTP Requests**: Total, taxa por segundo
- **Logback**: Logs por n√≠vel (INFO, WARN, ERROR)
- **Tomcat**: Sessions, threads do servidor
- **JVM Quick Stats**: CPU, mem√≥ria, threads

**Quando usar**:
- ‚úÖ Monitorar carga de requisi√ß√µes HTTP
- ‚úÖ Detectar picos de erro (Logback ERROR aumentando)
- ‚úÖ Verificar se Tomcat est√° saturado (threads no m√°ximo)

**Como interpretar**:
```
üü¢ HTTP 5xx < 1% ‚Üí OK
üü° HTTP 5xx 1-5% ‚Üí Aten√ß√£o (investigar causas)
üî¥ HTTP 5xx > 5% ‚Üí Cr√≠tico (servi√ßo degradado)

üü¢ Tomcat threads < 80% do m√°ximo ‚Üí OK
üü° Tomcat threads 80-95% ‚Üí Aten√ß√£o (pode precisar escalar)
üî¥ Tomcat threads = 100% ‚Üí Cr√≠tico (requisi√ß√µes sendo rejeitadas)
```

---

**3. Resilience4j** (`resilience4j_dashboard`)

**Para que serve**: Monitorar Circuit Breaker, Retry, Bulkhead

**Principais pain√©is**:
- **Circuit Breaker State**: CLOSED (verde), OPEN (vermelho), HALF_OPEN (amarelo)
- **Circuit Breaker Calls**: Successful vs Failed vs Not Permitted
- **Retry Attempts**: Quantas vezes retentou
- **Bulkhead Usage**: Capacidade do thread pool

**Quando usar**:
- ‚úÖ Investigar falhas de comunica√ß√£o com adquirente
- ‚úÖ Verificar se Circuit Breaker est√° protegendo corretamente
- ‚úÖ Analisar se retries est√£o funcionando

**Como interpretar**:
```
üü¢ CB State = CLOSED ‚Üí Sistema externo saud√°vel
üü° CB State = HALF_OPEN ‚Üí Testando recupera√ß√£o (normal ap√≥s falhas)
üî¥ CB State = OPEN ‚Üí Sistema externo down (fallback ativo)

üü¢ CB Failure Rate < 20% ‚Üí OK
üü° CB Failure Rate 20-50% ‚Üí Aten√ß√£o (instabilidade)
üî¥ CB Failure Rate > 50% ‚Üí Cr√≠tico (vai abrir)
```

---

#### **üìä Dashboards Customizados (2)**

**4. HTTP Metrics** (`http_metrics_toolschallenge`)

**Para que serve**: Analisar performance dos endpoints HTTP

**Principais pain√©is** (7):

1. **Request Rate by Endpoint**
   - O que mostra: Requisi√ß√µes/segundo por URI
   - Como usar: Identificar endpoints mais chamados
   - Exemplo: `/pagamentos` com 50 req/s vs `/estornos` com 5 req/s

2. **Latency Percentiles**
   - O que mostra: p50, p95, p99 de cada endpoint
   - Como usar: Detectar lentid√£o
   - Exemplo:
     ```
     /pagamentos:
       p50: 120ms ‚Üí 50% das requests < 120ms
       p95: 350ms ‚Üí 95% das requests < 350ms
       p99: 800ms ‚Üí 99% das requests < 800ms (pior caso)
     ```
   - **Interpreta√ß√£o**:
     - p50 √© a "experi√™ncia t√≠pica"
     - p95 √© o "SLA" (95% dos usu√°rios t√™m boa experi√™ncia)
     - p99 detecta "outliers" (casos ruins que afetam poucos usu√°rios)

3. **Error Rates (4xx vs 5xx)**
   - O que mostra: Taxa de erros separada por tipo
   - Como usar:
     - **4xx (vermelho)**: Erro do cliente (valida√ß√£o, not found) ‚Üí Normal em pequena quantidade
     - **5xx (laranja)**: Erro do servidor ‚Üí **NUNCA** deve ser alto
   - Exemplo:
     ```
     üü¢ 5xx = 0% ‚Üí Perfeito
     üî¥ 5xx = 5% ‚Üí CR√çTICO (investigar logs)
     ```

4. **Throughput by Endpoint**
   - O que mostra: Requests/segundo por endpoint E m√©todo (GET, POST, etc)
   - Como usar: Entender padr√£o de uso da API

5. **Success Rate Gauge**
   - O que mostra: % de requisi√ß√µes sem erro 5xx
   - Como usar: Meta = **>99%** (SLA t√≠pico)
   - Exemplo:
     ```
     üü¢ 99.9% ‚Üí Excelente (3 noves)
     üü° 99% ‚Üí Aceit√°vel (2 noves)
     üî¥ 95% ‚Üí Ruim (1 em cada 20 falha)
     ```

6. **Overall p99 Latency Gauge**
   - O que mostra: Lat√™ncia p99 global
   - Como usar: Meta = **< 1 segundo** para APIs REST
   - Exemplo:
     ```
     üü¢ p99 = 300ms ‚Üí √ìtimo
     üü° p99 = 800ms ‚Üí Aceit√°vel
     üî¥ p99 = 2s ‚Üí Ruim (usu√°rio percebe)
     ```

7. **Status Code Distribution (Pie Chart)**
   - O que mostra: Propor√ß√£o de 200, 201, 400, 404, 500, etc
   - Como usar: Vis√£o geral de sa√∫de da API
   - Exemplo esperado:
     ```
     200 OK: 70%
     201 Created: 20%
     400 Bad Request: 8%
     404 Not Found: 1.5%
     500 Error: 0.5% (DEVE ser < 1%)
     ```

**Quando usar este dashboard**:
- ‚úÖ Monitoramento di√°rio de performance
- ‚úÖ Investigar lentid√£o de API
- ‚úÖ Validar deploy (antes vs depois)
- ‚úÖ An√°lise de SLA (cumprir 99% de sucesso?)

---

**5. Business Metrics** (`business_metrics_toolschallenge`)

**Para que serve**: M√©tricas de neg√≥cio e sistema

**Principais pain√©is** (11):

1. **Pagamentos Rate by Status** (verde/vermelho/amarelo)
   - O que mostra: Taxa de cria√ß√£o de pagamentos por status
   - Cores:
     - üü¢ Verde: `AUTORIZADO` (bom)
     - üî¥ Vermelho: `NEGADO` (esperado, mas monitorar)
     - üü° Amarelo: `PENDENTE` (processando)
   - Como usar: Detectar se taxa de nega√ß√£o aumentou (problema no adquirente?)

2. **Estornos Rate by Status**
   - Similar ao anterior, para estornos
   - üü¢ `CANCELADO` (sucesso)
   - üî¥ `NEGADO` (problema)
   - üü° `PENDENTE`

3. **Circuit Breaker State Gauge** (0/1/2)
   - O que mostra:
     - **0 = CLOSED** (verde) ‚Üí Adquirente OK
     - **1 = OPEN** (vermelho) ‚Üí Adquirente DOWN
     - **2 = HALF_OPEN** (amarelo) ‚Üí Testando recupera√ß√£o
   - Como usar: Alertar equipe quando virar 1 (OPEN)

4. **DLQ Rate by Type**
   - O que mostra: Taxa de envio para Dead Letter Queue
   - Tags: `tipo=pagamento` ou `tipo=estorno`
   - Como usar:
     - DLQ crescendo = problemas de processamento
     - Investigar logs dos itens na DLQ

5. **DLQ Total Last Hour**
   - O que mostra: Total enviado para DLQ na √∫ltima hora
   - Como usar: Meta = **0** (ideal), < 10 aceit√°vel
   - Exemplo:
     ```
     üü¢ 0 itens ‚Üí Perfeito
     üü° 1-10 itens ‚Üí Aten√ß√£o (validar se √© transiente)
     üî¥ > 10 itens ‚Üí Investigar urgente
     ```

6-7. **Pagamento/Estorno Latency Percentiles**
   - O que mostra: p50/p95/p99 de cria√ß√£o de pagamento/estorno
   - Como usar: Detectar lentid√£o na l√≥gica de neg√≥cio

8-9. **Pagamentos/Estornos Last Hour (Stat Panels)**
   - O que mostra: Total criado na √∫ltima hora
   - Como usar: Monitorar volume de transa√ß√µes

10. **Pagamento - Taxa de Aprova√ß√£o (%)**
   - O que mostra: % de pagamentos autorizados (n√£o negados)
   - Como usar:
     - Meta t√≠pica: **> 70%** (depende do neg√≥cio)
     - Se cair muito: problema no adquirente ou fraudes?

11. **Estorno - Taxa de Sucesso (%)**
   - O que mostra: % de estornos cancelados com sucesso
   - Como usar: Meta = **> 95%** (estornos raramente devem falhar)

**Quando usar este dashboard**:
- ‚úÖ Monitoramento de neg√≥cio (KPIs)
- ‚úÖ Detectar anomalias (taxa de aprova√ß√£o caiu?)
- ‚úÖ Investigar DLQ (itens falhando?)
- ‚úÖ Validar resili√™ncia (Circuit Breaker funcionando?)

---

### **3. Dicas de Uso do Grafana**

#### **Time Range (canto superior direito)**

```
Last 5 minutes   ‚Üí Monitoramento em tempo real
Last 1 hour      ‚Üí Investigar problema recente
Last 24 hours    ‚Üí An√°lise di√°ria
Last 7 days      ‚Üí Tend√™ncias semanais
Custom range     ‚Üí An√°lise espec√≠fica (ex: hor√°rio do deploy)
```

#### **Auto-refresh**

- Dashboards configurados para **refresh a cada 5 segundos**
- √çcone de refresh ao lado do time range
- √ötil para deixar em monitor durante opera√ß√£o

#### **Variables (template variables)**

- `$application` = "toolschallenge" (filtrar m√©tricas da nossa app)
- Dropdown no topo do dashboard

#### **Zoom e Pan**

- **Zoom**: Arrastar no gr√°fico
- **Pan**: Shift + Arrastar
- **Reset zoom**: Clicar duas vezes no gr√°fico

#### **Inspect Panel**

- **Clicar no t√≠tulo do painel** ‚Üí **Inspect** ‚Üí **Data**
- Ver dados raw da query
- Copiar query PromQL para usar no Prometheus

---

## üî• Prometheus - Queries e M√©tricas

### **1. Acessar Prometheus**

```bash
http://localhost:9090
```

**Interface simples**: Campo de query PromQL + bot√£o "Execute"

---

### **2. PromQL B√°sico**

#### **Sintaxe de Query**

```promql
# M√©trica simples (√∫ltimo valor)
http_server_requests_seconds_count

# Filtrar por label
http_server_requests_seconds_count{uri="/pagamentos"}

# Filtrar m√∫ltiplos labels (AND)
http_server_requests_seconds_count{uri="/pagamentos", status="200"}

# Regex
http_server_requests_seconds_count{uri=~"/pagamentos.*"}

# Negar
http_server_requests_seconds_count{status!="200"}
```

---

### **3. Queries √öteis - Copia e Cola**

#### **üìä HTTP Metrics**

**1. Total de requisi√ß√µes por endpoint**
```promql
sum(http_server_requests_seconds_count{application="toolschallenge"}) by (uri)
```

**2. Taxa de requisi√ß√µes (req/segundo) √∫ltimos 5min**
```promql
sum(rate(http_server_requests_seconds_count{application="toolschallenge"}[5m])) by (uri)
```

**3. Lat√™ncia p95 por endpoint**
```promql
histogram_quantile(0.95, 
  sum(rate(http_server_requests_seconds_bucket{application="toolschallenge"}[5m])) by (le, uri)
)
```

**4. Taxa de erro 5xx**
```promql
sum(rate(http_server_requests_seconds_count{application="toolschallenge", status=~"5.."}[5m]))
```

**5. Taxa de sucesso (%)**
```promql
100 * (
  sum(rate(http_server_requests_seconds_count{application="toolschallenge", status!~"5.."}[5m])) 
  / 
  sum(rate(http_server_requests_seconds_count{application="toolschallenge"}[5m]))
)
```

---

#### **üíº Business Metrics**

**6. Pagamentos criados (√∫ltimos 5min)**
```promql
sum(rate(pagamento_criados_total{application="toolschallenge"}[5m])) by (status)
```

**7. Estornos criados (√∫ltimos 5min)**
```promql
sum(rate(estorno_criados_total{application="toolschallenge"}[5m])) by (status)
```

**8. Taxa de aprova√ß√£o de pagamentos (%)**
```promql
100 * (
  sum(rate(pagamento_criados_total{application="toolschallenge", status="AUTORIZADO"}[5m])) 
  / 
  sum(rate(pagamento_criados_total{application="toolschallenge"}[5m]))
)
```

**9. DLQ rate por tipo**
```promql
sum(rate(reprocessamento_dlq_total{application="toolschallenge"}[5m])) by (tipo)
```

**10. Total na DLQ (√∫ltima hora)**
```promql
sum(increase(reprocessamento_dlq_total{application="toolschallenge"}[1h])) by (tipo)
```

---

#### **üõ°Ô∏è Resili√™ncia**

**11. Estado do Circuit Breaker**
```promql
circuit_breaker_adquirente_state{application="toolschallenge"}
```
*Resultado: 0 = CLOSED, 1 = OPEN, 2 = HALF_OPEN*

**12. Taxa de falhas do Circuit Breaker**
```promql
resilience4j_circuitbreaker_failure_rate{name="adquirente"}
```

**13. Chamadas bloqueadas pelo CB**
```promql
sum(rate(resilience4j_circuitbreaker_calls_seconds_count{name="adquirente", kind="not_permitted"}[5m]))
```

**14. Retries executados**
```promql
sum(rate(resilience4j_retry_calls_seconds_count{name="adquirente"}[5m])) by (kind)
```

---

#### **üñ•Ô∏è JVM Metrics**

**15. Heap usado (MB)**
```promql
jvm_memory_used_bytes{application="toolschallenge", area="heap"} / 1024 / 1024
```

**16. Taxa de GC (collections/segundo)**
```promql
rate(jvm_gc_pause_seconds_count{application="toolschallenge"}[5m])
```

**17. Threads ativas**
```promql
jvm_threads_live_threads{application="toolschallenge"}
```

**18. Conex√µes DB ativas**
```promql
hikaricp_connections_active{pool="HikariPool-1"}
```

---

### **4. Dicas PromQL**

#### **Fun√ß√µes de Tempo**

```promql
rate(metric[5m])      # Taxa por segundo (para Counters)
increase(metric[1h])  # Incremento total (para Counters)
avg_over_time(metric[5m])  # M√©dia (para Gauges)
max_over_time(metric[5m])  # M√°ximo (para Gauges)
```

#### **Agrega√ß√µes**

```promql
sum(metric) by (label)     # Somar agrupando por label
avg(metric) by (label)     # M√©dia
max(metric)                # M√°ximo global
count(metric)              # Contar s√©ries
```

#### **Matem√°tica**

```promql
metric1 / metric2          # Divis√£o (para calcular %)
metric * 100               # Multiplica√ß√£o
metric1 - metric2          # Subtra√ß√£o
```

#### **Percentis (Histograms)**

```promql
histogram_quantile(0.50, ...)  # p50 (mediana)
histogram_quantile(0.95, ...)  # p95
histogram_quantile(0.99, ...)  # p99
```

---

### **5. Targets (Status de Scraping)**

**Menu**: Status ‚Üí Targets

**O que ver**:
- `toolschallenge-api` deve estar **UP** (verde)
- Last Scrape: < 30 segundos atr√°s
- Scrape Duration: < 100ms (normal)
- Errors: Vazio (se tiver erro, scraping falhou)

**Validar via API**:
```bash
curl http://localhost:9090/api/v1/targets
```

---

## üîç Jaeger - Distributed Tracing

### **1. Acessar Jaeger**

```bash
http://localhost:16686
```

---

### **2. Interface Jaeger**

**Tela principal**: Search Traces

**Campos**:
- **Service**: Selecionar `toolschallenge`
- **Operation**: Selecionar opera√ß√£o espec√≠fica (ex: `POST /pagamentos`)
- **Tags**: Filtros customizados (ex: `http.status_code=500`)
- **Lookback**: Quanto tempo atr√°s buscar (Last Hour, Last 24h, etc)

---

### **3. Buscar Traces**

#### **Caso 1: Ver todas as requisi√ß√µes recentes**

1. Service: `toolschallenge`
2. Lookback: **Last 1 Hour**
3. Clicar **Find Traces**

**Resultado**: Lista de traces com:
- TraceID (identificador √∫nico)
- Dura√ß√£o total (ex: 350ms)
- N√∫mero de spans (ex: 5 spans)
- Timestamp

---

#### **Caso 2: Buscar requisi√ß√µes lentas**

1. Service: `toolschallenge`
2. **Min Duration**: 500ms (s√≥ traces > 500ms)
3. Clicar **Find Traces**

**Resultado**: S√≥ traces lentos (√∫til para investigar lentid√£o)

---

#### **Caso 3: Buscar por Correlation ID**

1. **Tags**: `correlationId=d4c062ef-77ba-489f-9a05-86850c76fc90`
2. Clicar **Find Traces**

**Resultado**: Trace exato daquela requisi√ß√£o

**Quando usar**: Cliente reportou erro e voc√™ tem o Correlation ID do log

---

### **4. Analisar um Trace**

**Clicar em um trace da lista** ‚Üí Abre detalhes

**O que voc√™ v√™**:

#### **Timeline (Gantt Chart)**

Exemplo de trace de `POST /pagamentos`:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ http-server-span (350ms)                                ‚îÇ  ‚Üê Requisi√ß√£o HTTP total
‚îÇ   ‚îú‚îÄ PagamentoService.criar (280ms)                     ‚îÇ  ‚Üê L√≥gica de neg√≥cio
‚îÇ   ‚îÇ   ‚îú‚îÄ AdquirenteService.autorizarPagamento (200ms)   ‚îÇ  ‚Üê Chamada externa
‚îÇ   ‚îÇ   ‚îî‚îÄ Repository.save (50ms)                         ‚îÇ  ‚Üê Salvar no DB
‚îÇ   ‚îî‚îÄ OutboxService.salvar (30ms)                        ‚îÇ  ‚Üê Publicar evento
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Interpreta√ß√£o**:
- **Total**: 350ms
- **Gargalo**: `AdquirenteService.autorizarPagamento` (200ms = 57% do tempo)
- **Conclus√£o**: Lentid√£o vem do adquirente externo

---

#### **Span Details**

**Clicar em um span** ‚Üí Abre detalhes:

**Tags**:
- `http.method`: POST
- `http.url`: /pagamentos
- `http.status_code`: 201
- `correlationId`: d4c062ef-...

**Logs** (se houver):
- Eventos internos do span (ex: "Circuit Breaker ativado")

**Process** (metadata):
- `service.name`: toolschallenge
- `host.name`: fabio-pc
- `ip`: 192.168.1.10

---

### **5. Casos de Uso Jaeger**

#### **Caso 1: Requisi√ß√£o lenta - onde est√° o gargalo?**

**Problema**: Cliente reportou que POST /pagamentos demorou 5 segundos

**Solu√ß√£o**:
1. Buscar trace com Min Duration: 4000ms
2. Encontrar o trace de 5s
3. Analisar timeline:
   - Se `AdquirenteService` demorou 4.8s ‚Üí Problema no adquirente
   - Se `Repository.save` demorou 4.5s ‚Üí Problema no DB (query lenta?)
   - Se `OutboxService` demorou 4s ‚Üí Problema no Kafka?

---

#### **Caso 2: Erro 500 - qual m√©todo quebrou?**

**Problema**: Cliente recebeu 500 Internal Server Error

**Solu√ß√£o**:
1. Tags: `http.status_code=500`
2. Encontrar trace
3. Ver span com `error=true` (marcado em vermelho)
4. Ver logs do span para stacktrace

---

#### **Caso 3: Circuit Breaker aberto - quantas requests afetadas?**

**Problema**: Circuit Breaker abriu, quantas requests usaram fallback?

**Solu√ß√£o**:
1. Tags: `resilience4j.circuitbreaker.name=adquirente`
2. Ver quantos traces t√™m span de fallback
3. Analisar dura√ß√£o (fallback deve ser r√°pido, ex: 50ms)

---

### **6. Correlation ID - Rastreabilidade E2E**

#### **Como funciona**:

Toda requisi√ß√£o HTTP gera um **Correlation ID √∫nico** que:
- Aparece nos **logs** (facilita busca)
- Aparece no **Jaeger** como tag (permite buscar trace)
- √â propagado para **chamadas externas** (microservi√ßos)

#### **Exemplo pr√°tico**:

**1. Cliente faz requisi√ß√£o**:
```bash
curl -X POST http://localhost:8080/pagamentos \
  -H "Content-Type: application/json" \
  -H "Idempotency-Key: 550e8400-e29b-41d4-a716-446655440000" \
  -d '{"descricao":"Test","valor":100.00,"tipoPagamento":"CARTAO_CREDITO"}'
```

**2. Aplica√ß√£o gera Correlation ID**: `d4c062ef-77ba-489f-9a05-86850c76fc90`

**3. Logs mostram o ID**:
```
2025-11-03 14:30:15 INFO [d4c062ef] PagamentoService - Criando pagamento
2025-11-03 14:30:15 INFO [d4c062ef] AdquirenteService - Autorizando pagamento
2025-11-03 14:30:16 ERROR [d4c062ef] AdquirenteService - Timeout ao chamar adquirente
```

**4. Buscar no Jaeger**: `correlationId=d4c062ef-77ba-489f-9a05-86850c76fc90`

**Benef√≠cio**: Rastreabilidade completa de uma requisi√ß√£o (logs + traces + m√©tricas)

---

## ü©∫ Actuator - Health Checks

### **1. Acessar Actuator**

**Base URL**: http://localhost:8080/atuador

**Endpoints principais**:

```bash
# Status geral
http://localhost:8080/atuador/health

# M√©tricas dispon√≠veis
http://localhost:8080/atuador/metrics

# M√©trica espec√≠fica
http://localhost:8080/atuador/metrics/http.server.requests

# M√©tricas Prometheus
http://localhost:8080/atuador/prometheus

# Circuit Breakers
http://localhost:8080/atuador/circuitbreakers

# Hist√≥rico de eventos CB
http://localhost:8080/atuador/circuitbreakerevents
```

---

### **2. Health Endpoint**

```bash
curl http://localhost:8080/atuador/health | jq
```

**Resposta exemplo**:
```json
{
  "status": "UP",
  "components": {
    "db": {
      "status": "UP",
      "details": {
        "database": "PostgreSQL",
        "validationQuery": "isValid()"
      }
    },
    "redis": {
      "status": "UP",
      "details": {
        "version": "7.4.6"
      }
    },
    "diskSpace": {
      "status": "UP",
      "details": {
        "total": 500000000000,
        "free": 250000000000,
        "threshold": 10485760,
        "exists": true
      }
    },
    "ping": {
      "status": "UP"
    }
  }
}
```

**Interpreta√ß√£o**:
- `status: UP` ‚Üí Aplica√ß√£o saud√°vel
- `status: DOWN` ‚Üí Algum componente falhou (DB, Redis, etc)

**Quando usar**:
- ‚úÖ Health checks de load balancer
- ‚úÖ Monitoramento de uptime
- ‚úÖ Validar que DB/Redis est√£o acess√≠veis

---

### **3. Circuit Breakers Endpoint**

```bash
curl http://localhost:8080/atuador/circuitbreakers | jq
```

**Resposta exemplo**:
```json
{
  "circuitBreakers": {
    "adquirente": {
      "state": "HALF_OPEN",
      "failureRate": "45.5%",
      "slowCallRate": "10.2%",
      "bufferedCalls": 10,
      "failedCalls": 5,
      "slowCalls": 1,
      "notPermittedCalls": 3
    }
  }
}
```

**Interpreta√ß√£o**:
- `state: CLOSED` ‚Üí Tudo OK
- `state: OPEN` ‚Üí Sistema externo down, fallback ativo
- `state: HALF_OPEN` ‚Üí Testando recupera√ß√£o
- `failureRate`: Taxa de falhas (> 50% abre o CB)
- `notPermittedCalls`: Chamadas bloqueadas (CB OPEN)

---

### **4. Metrics Endpoint**

**Listar todas as m√©tricas**:
```bash
curl http://localhost:8080/atuador/metrics | jq '.names'
```

**Ver m√©trica espec√≠fica**:
```bash
curl http://localhost:8080/atuador/metrics/http.server.requests | jq
```

**Resposta** (truncada):
```json
{
  "name": "http.server.requests",
  "measurements": [
    {"statistic": "COUNT", "value": 1523.0},
    {"statistic": "TOTAL_TIME", "value": 45.234},
    {"statistic": "MAX", "value": 2.1}
  ],
  "availableTags": [
    {"tag": "uri", "values": ["/pagamentos", "/estornos", "/atuador/health"]},
    {"tag": "status", "values": ["200", "201", "400", "404", "500"]},
    {"tag": "method", "values": ["GET", "POST"]}
  ]
}
```

---

## üéØ Casos de Uso Pr√°ticos

### **Caso 1: "A API est√° lenta!"**

**Objetivo**: Identificar qual endpoint est√° lento e por qu√™

**Passo a Passo**:

1. **Grafana** ‚Üí **HTTP Metrics Dashboard**
   - Ver painel **Latency Percentiles**
   - Identificar endpoint com p99 > 1s

2. **Prometheus** ‚Üí Query espec√≠fica:
   ```promql
   histogram_quantile(0.99, 
     sum(rate(http_server_requests_seconds_bucket{uri="/pagamentos"}[5m])) by (le)
   )
   ```
   - Confirmar lat√™ncia alta (ex: 2.5s)

3. **Jaeger** ‚Üí Buscar traces lentos:
   - Service: `toolschallenge`
   - Operation: `POST /pagamentos`
   - Min Duration: `2000ms`
   - Analisar timeline para identificar gargalo

4. **Resultado**: Descobrir que `AdquirenteService` est√° demorando 2.3s (92% do tempo)

5. **A√ß√£o**: Investigar logs do adquirente, verificar Circuit Breaker, considerar timeout mais curto

---

### **Caso 2: "Muitos pagamentos negados!"**

**Objetivo**: Analisar por que taxa de aprova√ß√£o caiu de 80% para 50%

**Passo a Passo**:

1. **Grafana** ‚Üí **Business Metrics Dashboard**
   - Ver painel **Pagamento - Taxa de Aprova√ß√£o**
   - Confirmar queda (ex: de 80% para 52%)
   - Ver **Pagamentos Rate by Status**:
     - AUTORIZADO: 10/s (normal)
     - NEGADO: 15/s (dobrou!) ‚Üê Problema aqui

2. **Prometheus** ‚Üí Query para confirmar:
   ```promql
   sum(rate(pagamento_criados_total{status="NEGADO"}[5m]))
   ```
   - Resultado: 0.25/s (normal era 0.10/s)

3. **Logs** ‚Üí Buscar logs de nega√ß√£o:
   ```bash
   grep "NEGADO" application.log | tail -50
   ```
   - Ver padr√£o: "Saldo insuficiente" vs "Cart√£o bloqueado"

4. **Jaeger** ‚Üí Buscar traces de pagamentos negados:
   - Tags: `http.url=/pagamentos` AND `status=NEGADO`
   - Analisar response do adquirente

5. **Resultado**: Adquirente retornando "fraude detectada" em massa (poss√≠vel falso positivo)

6. **A√ß√£o**: Contatar adquirente, ajustar regras de fraude

---

### **Caso 3: "Circuit Breaker est√° abrindo muito!"**

**Objetivo**: Entender por que Circuit Breaker est√° abrindo e se est√° protegendo corretamente

**Passo a Passo**:

1. **Grafana** ‚Üí **Resilience4j Dashboard**
   - Ver **Circuit Breaker State** (gauge): Estado = 1 (OPEN) ‚Üê Confirmado
   - Ver **Circuit Breaker Calls**:
     - Failed: 80%
     - Successful: 10%
     - Not Permitted: 10% (bloqueadas pelo CB)

2. **Actuator** ‚Üí Status do CB:
   ```bash
   curl http://localhost:8080/atuador/circuitbreakers
   ```
   - `failureRate: 82.5%` (threshold √© 50%, por isso abriu)
   - `state: OPEN`

3. **Prometheus** ‚Üí Query hist√≥rica:
   ```promql
   circuit_breaker_adquirente_state
   ```
   - Graph ‚Üí Ver quando mudou de 0 (CLOSED) para 1 (OPEN)
   - Exemplo: Abriu √†s 14:35, ficou em OPEN por 10s, foi para HALF_OPEN

4. **Logs** ‚Üí Ver quando CB abriu:
   ```bash
   grep "Circuit Breaker" application.log
   ```
   - Logs: "Circuit Breaker OPEN - Adquirente indispon√≠vel"

5. **Jaeger** ‚Üí Ver traces com fallback:
   - Tags: `resilience4j.circuitbreaker.name=adquirente`
   - Ver quanto tempo durou fallback (deve ser r√°pido, ex: 50ms)

6. **Resultado**: Adquirente teve instabilidade √†s 14:35 (82% de falhas), CB protegeu corretamente

7. **A√ß√£o**: Verificar se adquirente se recuperou, ajustar threshold se necess√°rio

---

### **Caso 4: "DLQ est√° crescendo!"**

**Objetivo**: Investigar por que itens est√£o indo para Dead Letter Queue

**Passo a Passo**:

1. **Grafana** ‚Üí **Business Metrics Dashboard**
   - Ver **DLQ Total Last Hour**: 45 itens (anormal, normal √© 0-5)
   - Ver **DLQ Rate by Type**:
     - `tipo=pagamento`: 0.5/s
     - `tipo=estorno`: 0.1/s

2. **Prometheus** ‚Üí Query:
   ```promql
   sum(increase(reprocessamento_dlq_total[1h])) by (tipo)
   ```
   - Resultado: pagamento=30, estorno=15

3. **Logs** ‚Üí Ver por que foram para DLQ:
   ```bash
   grep "Enviando para DLQ" application.log
   ```
   - Logs: "Tentativas esgotadas (3/3): Timeout ao chamar adquirente"

4. **Database** ‚Üí Query na tabela `pagamento`:
   ```sql
   SELECT id, status, tentativas_processamento 
   FROM pagamento 
   WHERE status = 'PENDENTE' AND tentativas_processamento >= 3;
   ```
   - Ver IDs espec√≠ficos que falharam

5. **Jaeger** ‚Üí Buscar trace de um item da DLQ:
   - TraceID do log
   - Ver onde quebrou (timeout no AdquirenteService)

6. **Resultado**: Adquirente teve timeout em todas as 3 tentativas de retry

7. **A√ß√£o**:
   - Verificar se adquirente est√° saud√°vel
   - Considerar aumentar timeout (atual 5s ‚Üí 10s?)
   - Reprocessar manualmente itens da DLQ ap√≥s fix

---

### **Caso 5: "JVM com OutOfMemoryError!"**

**Objetivo**: Identificar memory leak antes do OOM

**Passo a Passo**:

1. **Grafana** ‚Üí **JVM Micrometer Dashboard**
   - Ver **JVM Memory Pools**:
     - Heap Used: 1.8GB / 2GB (90% usado!) ‚Üê Problema
     - Old Gen: 1.6GB (crescendo continuamente)
   - Ver **Garbage Collection**:
     - GC Count: 50/min (muito alto!)
     - GC Pause Time: 800ms (usu√°rio vai sentir)

2. **Prometheus** ‚Üí Query:
   ```promql
   jvm_memory_used_bytes{area="heap"} / jvm_memory_max_bytes{area="heap"}
   ```
   - Resultado: 0.92 (92% de heap usado)

3. **Actuator** ‚Üí For√ßar GC (teste):
   ```bash
   curl -X POST http://localhost:8080/atuador/gc
   ```
   - Se heap n√£o diminuir significativamente = memory leak

4. **Heap Dump** (para an√°lise profunda):
   ```bash
   jmap -dump:live,format=b,file=heap.bin <PID>
   ```
   - Analisar com Eclipse MAT ou VisualVM

5. **Resultado**: Encontrar classes com muitas inst√¢ncias (ex: 100k objetos `PagamentoDTO`)

6. **A√ß√£o**:
   - Investigar c√≥digo (cache infinito?)
   - Aumentar heap temporariamente (-Xmx4g)
   - Fixar leak

---

## üõ†Ô∏è Troubleshooting com Observabilidade

### **Problema 1: "N√£o vejo m√©tricas no Grafana"**

**Checklist**:

1. **Aplica√ß√£o est√° rodando?**
   ```bash
   curl http://localhost:8080/atuador/health
   ```
   - Se erro ‚Üí Aplica√ß√£o down

2. **Prometheus est√° coletando?**
   - Acessar http://localhost:9090/targets
   - `toolschallenge-api` deve estar UP (verde)
   - Se DOWN ‚Üí Verificar firewall, endere√ßo (host.docker.internal)

3. **M√©tricas est√£o sendo exportadas?**
   ```bash
   curl http://localhost:8080/atuador/prometheus
   ```
   - Deve retornar texto com m√©tricas (ex: `http_server_requests_seconds_count{...}`)

4. **Grafana est√° conectado ao Prometheus?**
   - Grafana ‚Üí Configuration ‚Üí Data Sources ‚Üí Prometheus
   - URL deve ser `http://prometheus:9090`
   - Clicar "Test" ‚Üí Deve aparecer "Data source is working"

5. **Dashboard tem dados?**
   - Verificar time range (√∫ltimas 5min? √∫ltima 1h?)
   - Verificar filtro `$application` = "toolschallenge"

---

### **Problema 2: "N√£o vejo traces no Jaeger"**

**Checklist**:

1. **Aplica√ß√£o est√° enviando traces?**
   - Ver logs de startup:
     ```
     Micrometer Tracing enabled
     Jaeger exporter configured
     ```

2. **Jaeger est√° rodando?**
   ```bash
   curl http://localhost:16686/api/services
   ```
   - Deve retornar lista de services

3. **Service `toolschallenge` aparece?**
   - Se n√£o aparecer ‚Üí Aplica√ß√£o n√£o est√° enviando traces
   - Verificar configura√ß√£o:
     ```yaml
     management:
       tracing:
         sampling:
           probability: 1.0  # 100% de sampling
     ```

4. **Fazer requisi√ß√£o e buscar trace**:
   ```bash
   curl -X POST http://localhost:8080/pagamentos \
     -H "Idempotency-Key: $(uuidgen)" \
     -H "Content-Type: application/json" \
     -d '{"descricao":"Test","valor":100,"tipoPagamento":"CARTAO_CREDITO"}'
   ```
   - Buscar no Jaeger (Lookback: Last 5 minutes)

---

### **Problema 3: "Circuit Breaker n√£o abre mesmo com falhas"**

**Checklist**:

1. **Verificar configura√ß√£o**:
   ```yaml
   resilience4j:
     circuitbreaker:
       instances:
         adquirente:
           failure-rate-threshold: 50  # 50% de falhas
           minimum-number-of-calls: 5   # Precisa de 5 chamadas antes de calcular
   ```

2. **Gerar falhas suficientes**:
   - CB s√≥ calcula taxa ap√≥s `minimum-number-of-calls` (5)
   - Fazer pelo menos 5 requisi√ß√µes
   - Pelo menos 3 devem falhar (50%)

3. **Ver estado no Actuator**:
   ```bash
   curl http://localhost:8080/atuador/circuitbreakers
   ```

4. **Ver gauge no Prometheus**:
   ```promql
   circuit_breaker_adquirente_state
   ```
   - 0 = CLOSED, 1 = OPEN, 2 = HALF_OPEN

---

## üìö Resumo - Cheat Sheet

### **Acesso R√°pido**

```bash
# Grafana (dashboards visuais)
http://localhost:3000
admin / admin123

# Prometheus (queries PromQL)
http://localhost:9090

# Jaeger (distributed tracing)
http://localhost:16686

# Actuator (health checks)
http://localhost:8080/atuador/health

# Swagger (documenta√ß√£o API)
http://localhost:8080/swagger-ui.html
```

### **Queries PromQL Top 5**

```promql
# 1. Taxa de requisi√ß√µes HTTP
sum(rate(http_server_requests_seconds_count[5m])) by (uri)

# 2. Lat√™ncia p99
histogram_quantile(0.99, sum(rate(http_server_requests_seconds_bucket[5m])) by (le, uri))

# 3. Taxa de erro 5xx
sum(rate(http_server_requests_seconds_count{status=~"5.."}[5m]))

# 4. Estado do Circuit Breaker
circuit_breaker_adquirente_state

# 5. Pagamentos criados
sum(rate(pagamento_criados_total[5m])) by (status)
```

### **Dashboards Essenciais**

1. **HTTP Metrics** ‚Üí Monitoramento di√°rio de performance
2. **Business Metrics** ‚Üí KPIs de neg√≥cio e resili√™ncia
3. **JVM Micrometer** ‚Üí Sa√∫de da JVM (memory, GC, threads)
4. **Resilience4j** ‚Üí Circuit Breaker, Retry, Bulkhead

### **Fluxo de Investiga√ß√£o**

```
Problema reportado
    ‚Üì
1. Grafana (vis√£o geral - dashboard relevante)
    ‚Üì
2. Prometheus (query espec√≠fica para confirmar)
    ‚Üì
3. Jaeger (trace individual para debugar)
    ‚Üì
4. Logs (detalhes finais - stacktrace, mensagens)
```

---

## üéì Pr√≥ximos Passos

### **Para Aprender Mais**

1. **PromQL**:
   - [Prometheus Query Basics](https://prometheus.io/docs/prometheus/latest/querying/basics/)
   - [PromQL Examples](https://prometheus.io/docs/prometheus/latest/querying/examples/)

2. **Grafana**:
   - [Grafana Dashboards Best Practices](https://grafana.com/docs/grafana/latest/dashboards/)
   - [Grafana Variables](https://grafana.com/docs/grafana/latest/dashboards/variables/)

3. **Jaeger**:
   - [Jaeger Architecture](https://www.jaegertracing.io/docs/1.50/architecture/)
   - [OpenTelemetry Tracing](https://opentelemetry.io/docs/concepts/observability-primer/#distributed-traces)

4. **Resilience4j**:
   - [Circuit Breaker Pattern](https://resilience4j.readme.io/docs/circuitbreaker)
   - [Metrics Integration](https://resilience4j.readme.io/docs/micrometer)

---

**Autor**: ToolsChallenge Team  
**Data**: 03/11/2025  
**Vers√£o**: 1.0
